{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f729e9fa-ec3b-4fa4-99b5-fce4f596a0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets sentencepiece sacrebleu accelerate gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bb3b0-171b-4c5a-aa00-9465c2558059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 10:57:11.510466: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-26 10:57:11.523131: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745645231.537519    9107 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745645231.542050    9107 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745645231.553443    9107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745645231.553461    9107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745645231.553462    9107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745645231.553464    9107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-26 10:57:11.557154: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "DataFrame shape: (547567, 2)\n",
      "Sample data:\n",
      "                                                Tamil  \\\n",
      "0     ‡ÆÖ‡Æµ‡Æ≥‡Øç ‡Æ™‡ØÜ‡ÆØ‡Æ∞‡Øç ‡Æï‡ØÇ‡Æü ‡ÆÖ‡Æµ‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æí‡Æ©‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æ®‡Æø‡Æ©‡Øà‡Æµ‡Æø‡Æ≤‡Øç ‡Æá‡Æ≤‡Øç‡Æ≤‡Øà   \n",
      "1  ‡Æö‡ÆÆ‡Øà‡Æ™‡Øç‡Æ™‡Æ§‡ØÅ ‡Æµ‡Øá‡Æï‡ÆÆ‡Ææ‡Æ©‡Æ§‡ØÅ ‡Æá‡Æ§‡Æ©‡Øç ‡Æµ‡Æø‡Æ≥‡Øà‡Æµ‡Ææ‡Æï ‡Æä‡Æü‡Øç‡Æü‡Æö‡Øç‡Æö‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥...   \n",
      "2  ‡Æ®‡Ææ‡ÆÆ‡Øç ‡Æè‡Æ±‡Øç‡Æï‡Æ©‡Æµ‡Øá ‡Æö‡ØÜ‡ÆØ‡Øç‡Æµ‡Æ§‡Øà ‡Æ∞‡Æö‡Æø‡Æ™‡Øç‡Æ™‡Æ§‡Øà‡Æï‡Øç ‡Æï‡Æ£‡Øç‡Æü‡ØÅ‡Æ™‡Æø‡Æü‡Æø‡Æ™‡Øç‡Æ™‡Æ§‡Æ±...   \n",
      "3  ‡Æá‡Æ§‡ØÅ ‡Æí‡Æ∞‡ØÅ ‡ÆÆ‡Øá‡Æ©‡ØÅ‡Æµ‡Æ≤‡Øç ‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ ‡ÆÜ‡Æü‡Øç‡Æü‡Øã‡ÆÆ‡Øá‡Æü‡Øç‡Æü‡Æø‡Æï‡Øç ‡Æï‡Æø‡ÆØ‡Æ∞‡Øç‡Æ™‡Ææ‡Æï‡Øç...   \n",
      "4                           ‡Æá‡Æ§‡ØÅ‡Æµ‡ØÅ‡ÆÆ‡Øç ‡Æ®‡Æ≤‡Øç‡Æ≤ ‡ÆÆ‡ØÅ‡ÆØ‡Æ±‡Øç‡Æö‡Æø‡Æ§‡Ææ‡Æ©‡Øç   \n",
      "\n",
      "                                              Telugu  \n",
      "0               ‡∞ï‡∞®‡±Ä‡∞∏‡∞Ç ‡∞Ü‡∞Æ‡±Ü ‡∞™‡±á‡∞∞‡±Å ‡∞ï‡±Ç‡∞°‡∞æ ‡∞Ü‡∞Ø‡∞®‡∞ï‡±Å ‡∞ó‡±Å‡∞∞‡±ç‡∞§‡±Å‡∞≤‡±á‡∞¶‡±Å  \n",
      "1  ‡∞µ‡∞Ç‡∞ü ‡∞µ‡±á‡∞ó‡∞Ç‡∞ó‡∞æ ‡∞â‡∞Ç‡∞ü‡±Å‡∞Ç‡∞¶‡∞ø ‡∞§‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ ‡∞™‡±ã‡∞∑‡∞ï‡∞æ‡∞≤‡±Å ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞µ‡∞ø‡∞ü‡∞Æ‡∞ø...  \n",
      "2  ‡∞Æ‡∞®‡∞Ç ‡∞á‡∞™‡±ç‡∞™‡∞ü‡∞ø‡∞ï‡±á ‡∞Ü‡∞®‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡±á‡∞¶‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞ó‡±Å‡∞∞‡±ç‡∞§‡∞ø‡∞Ç‡∞ö‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞¨‡∞¶‡±Å...  \n",
      "3  ‡∞á‡∞¶‡∞ø ‡∞Æ‡∞æ‡∞®‡±ç‡∞Ø‡±Å‡∞µ‡∞≤‡±ç ‡∞≤‡±á‡∞¶‡∞æ ‡∞Ü‡∞ü‡±ã‡∞Æ‡±á‡∞ü‡∞ø‡∞ï‡±ç ‡∞ó‡±á‡∞∞‡±ç‡∞¨‡∞æ‡∞ï‡±ç‡∞∏‡±ç‡∞§‡±ã ‡∞™‡±Ü‡∞ü‡±ç...  \n",
      "4                ‡∞á‡∞¶‡∞ø ‡∞ï‡±Ç‡∞°‡∞æ ‡∞Æ‡∞Ç‡∞ö‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó ‡∞ï‡∞∞‡∞Æ‡±à‡∞® ‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞Æ‡±á  \n",
      "Train dataset size: 492810\n",
      "Test dataset size: 54757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mBART - Initial tokenizer vocab size: 250054\n",
      "mBART - Initial model output vocab size: 250054\n",
      "mBART - Vocab sizes match, no adjustment needed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dddefecfe2e4f4983e3f58a9690612e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/492810 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c020ae2616f466fbfc42e3526009fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mBART - Tokenized train sample: {'input_ids': [250044, 2690, 3770, 63277, 235753, 8182, 15453, 483, 55963, 86322, 78611, 8285, 6149, 80334, 8182, 131846, 8182, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [250045, 60078, 1296, 483, 6, 136571, 27013, 14206, 4276, 103646, 95432, 8197, 55763, 5271, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n",
      "Training mBART...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77005' max='77005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77005/77005 9:59:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.282100</td>\n",
       "      <td>1.240659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.064900</td>\n",
       "      <td>1.125419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>1.088127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.805200</td>\n",
       "      <td>1.072729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.710800</td>\n",
       "      <td>1.076244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mBART - Saved tokenizer vocab size: 250054\n",
      "mBART - Saved model output vocab size: 250054\n",
      "Tokenized Input IDs: [[250044, 190574, 4, 37305, 29947, 128251, 73952, 32, 2]]\n",
      "Raw Output IDs: [2, 250045, 9327, 3071, 1886, 89838, 4, 22735, 24722, 91064, 32, 2]\n",
      "Decoded with special tokens: </s>te_IN ‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å?</s>\n",
      "mBART Translation: ‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å?\n",
      "Training log plot saved as 'training_loss_plot.png' in the output directory.\n"
     ]
    }
   ],
   "source": [
    "#mBART3 with BASE\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import MBart50TokenizerFast, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "import logging\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Set up logging to file\n",
    "log_file = os.path.join(\"./mbart_finetuned3\", \"training_logs.csv\")\n",
    "os.makedirs(\"./mbart_finetuned3\", exist_ok=True)\n",
    "\n",
    "# Initialize CSV log file with headers\n",
    "with open(log_file, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"epoch\", \"step\", \"training_loss\", \"validation_loss\", \"learning_rate\", \"timestamp\"])\n",
    "\n",
    "# Custom callback for logging\n",
    "class CustomLoggingCallback(transformers.TrainerCallback):\n",
    "    def __init__(self, log_file):\n",
    "        self.log_file = log_file\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is None:\n",
    "            return\n",
    "        # Extract relevant metrics\n",
    "        epoch = state.epoch\n",
    "        step = state.global_step\n",
    "        training_loss = logs.get(\"loss\", None)\n",
    "        validation_loss = logs.get(\"eval_loss\", None)\n",
    "        learning_rate = logs.get(\"learning_rate\", None)\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Write to CSV\n",
    "        with open(self.log_file, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([epoch, step, training_loss, validation_loss, learning_rate, timestamp])\n",
    "\n",
    "# Load DataFrame\n",
    "df = pd.read_csv('./merged_output.csv')\n",
    "df1 = df.copy()\n",
    "\n",
    "\n",
    "# Verify DataFrame\n",
    "print(\"DataFrame shape:\", df1.shape)\n",
    "print(\"Sample data:\\n\", df1.head(5))\n",
    "\n",
    "# Rename columns\n",
    "df1 = df1.rename(columns={\"Tamil\": \"ta\", \"Telugu\": \"te\"})\n",
    "\n",
    "# Convert to Dataset\n",
    "dataset = Dataset.from_pandas(df1)\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "split_dataset = DatasetDict({\"train\": split_dataset[\"train\"], \"test\": split_dataset[\"test\"]})\n",
    "print(\"Train dataset size:\", len(split_dataset[\"train\"]))\n",
    "print(\"Test dataset size:\", len(split_dataset[\"test\"]))\n",
    "\n",
    "# Load mBART model and tokenizer\n",
    "MBART_MODEL_NAME = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "mbart_tokenizer = MBart50TokenizerFast.from_pretrained(MBART_MODEL_NAME, src_lang=\"ta_IN\", tgt_lang=\"te_IN\")\n",
    "mbart_model = AutoModelForSeq2SeqLM.from_pretrained(MBART_MODEL_NAME).to(device)\n",
    "\n",
    "# Verify vocab sizes\n",
    "mbart_vocab_size_tokenizer = len(mbart_tokenizer)\n",
    "mbart_vocab_size_model = mbart_model.get_output_embeddings().weight.size(0)\n",
    "print(\"mBART - Initial tokenizer vocab size:\", mbart_vocab_size_tokenizer)\n",
    "print(\"mBART - Initial model output vocab size:\", mbart_vocab_size_model)\n",
    "\n",
    "# Handle vocab size mismatch\n",
    "if mbart_vocab_size_tokenizer != mbart_vocab_size_model:\n",
    "    print(f\"Warning: mBART vocab size mismatch (Tokenizer: {mbart_vocab_size_tokenizer}, Model: {mbart_vocab_size_model}). Adjusting model embeddings.\")\n",
    "    mbart_model.resize_token_embeddings(mbart_vocab_size_tokenizer)\n",
    "    print(\"Post-resize model vocab size:\", mbart_model.get_output_embeddings().weight.size(0))\n",
    "else:\n",
    "    print(\"mBART - Vocab sizes match, no adjustment needed.\")\n",
    "\n",
    "# Preprocessing function\n",
    "def mbart_preprocess_function(examples):\n",
    "    inputs = [ta_text for ta_text in examples[\"ta\"]]\n",
    "    targets = [te_text for te_text in examples[\"te\"]]\n",
    "    model_inputs = mbart_tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with mbart_tokenizer.as_target_tokenizer():\n",
    "        labels = mbart_tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
    "    labels = [[-100 if token == mbart_tokenizer.pad_token_id else token for token in seq] for seq in labels]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "mbart_tokenized_datasets = split_dataset.map(\n",
    "    mbart_preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    remove_columns=[\"ta\", \"te\"]\n",
    ")\n",
    "print(\"mBART - Tokenized train sample:\", mbart_tokenized_datasets[\"train\"][0])\n",
    "\n",
    "# Training arguments\n",
    "mbart_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbart_finetuned3\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=5,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    logging_steps=500,\n",
    "    save_steps=5000\n",
    ")\n",
    "\n",
    "# Data collator and trainer\n",
    "mbart_data_collator = DataCollatorForSeq2Seq(mbart_tokenizer, model=mbart_model)\n",
    "mbart_trainer = Seq2SeqTrainer(\n",
    "    model=mbart_model,\n",
    "    args=mbart_training_args,\n",
    "    train_dataset=mbart_tokenized_datasets[\"train\"],\n",
    "    eval_dataset=mbart_tokenized_datasets[\"test\"],\n",
    "    tokenizer=mbart_tokenizer,\n",
    "    data_collator=mbart_data_collator,\n",
    "    callbacks=[CustomLoggingCallback(log_file)]\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Training mBART...\")\n",
    "mbart_trainer.train()\n",
    "\n",
    "# Save\n",
    "mbart_trainer.save_model(\"./mbart_finetuned3\")\n",
    "mbart_tokenizer.save_pretrained(\"./mbart_finetuned3\")\n",
    "\n",
    "# Verify saved model\n",
    "mbart_saved_model = AutoModelForSeq2SeqLM.from_pretrained(\"./mbart_finetuned3\").to(device)\n",
    "mbart_saved_tokenizer = MBart50TokenizerFast.from_pretrained(\"./mbart_finetuned3\", src_lang=\"ta_IN\", tgt_lang=\"te_IN\")\n",
    "print(\"mBART - Saved tokenizer vocab size:\", len(mbart_saved_tokenizer))\n",
    "print(\"mBART - Saved model output vocab size:\", mbart_saved_model.get_output_embeddings().weight.size(0))\n",
    "\n",
    "# Test translation with debugging\n",
    "def mbart_translate_text(input_text, debug=False):\n",
    "    inputs = mbart_saved_tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).to(device)\n",
    "    if debug:\n",
    "        print(\"Tokenized Input IDs:\", inputs[\"input_ids\"].tolist())\n",
    "    outputs = mbart_saved_model.generate(\n",
    "        **inputs,\n",
    "        max_length=256,\n",
    "        min_length=10,\n",
    "        num_beams=5,\n",
    "        early_stopping=False,\n",
    "        length_penalty=1.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        forced_bos_token_id=mbart_saved_tokenizer.lang_code_to_id[\"te_IN\"]\n",
    "    )\n",
    "    if debug:\n",
    "        print(\"Raw Output IDs:\", outputs[0].tolist())\n",
    "        print(\"Decoded with special tokens:\", mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=False))\n",
    "    decoded_output = mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output.strip()\n",
    "\n",
    "# Test\n",
    "input_text = \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç, ‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡ØÄ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç?\"  # \"Hello, how are you?\"\n",
    "translated_text = mbart_translate_text(input_text, debug=True)\n",
    "print(\"mBART Translation:\", translated_text)\n",
    "\n",
    "# Plotting script for training logs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_logs(log_file):\n",
    "    # Read the CSV log file\n",
    "    logs = pd.read_csv(log_file)\n",
    "    \n",
    "    # Filter rows with non-null training and validation loss\n",
    "    train_logs = logs[logs['training_loss'].notnull()]\n",
    "    valid_logs = logs[logs['validation_loss'].notnull()]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if not train_logs.empty:\n",
    "        plt.plot(train_logs['step'], train_logs['training_loss'], label='Training Loss', marker='o')\n",
    "    if not valid_logs.empty:\n",
    "        plt.plot(valid_logs['step'], valid_logs['validation_loss'], label='Validation Loss', marker='s')\n",
    "    \n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(\"./mbart_finetuned3\", \"training_loss_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Generate plot\n",
    "plot_training_logs(log_file)\n",
    "print(\"Training log plot saved as 'training_loss_plot.png' in the output directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad33da9-a2db-46dc-98e2-cd881f565828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 15:47:57.253181: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-05 15:47:57.264918: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746440277.278350   83313 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746440277.282536   83313 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746440277.292606   83313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746440277.292620   83313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746440277.292621   83313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746440277.292623   83313 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-05 15:47:57.296357: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Tamil to Telugu Translator</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Tamil text below to translate to Telugu (type 'exit' to stop):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamil Input:  ‡ÆÖ‡ÆÆ‡ØÜ‡Æ∞‡Æø‡Æï‡Øç‡Æï‡Ææ‡Æµ‡Æø‡Æ≤‡Øç ‡Æµ‡Æü‡Øç‡Æü‡Æø ‡Æµ‡Æø‡Æï‡Æø‡Æ§‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡ÆÖ‡Æ§‡Æø‡Æï‡Æ∞‡Æø‡Æ§‡Øç‡Æ§‡Ææ‡Æ≤‡Øç, ‡Æâ‡Æ£‡Æ∞‡Øç‡Æµ‡ØÅ ‡Æö‡Æ®‡Øç‡Æ§‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡Æ§‡Æô‡Øç‡Æï ‡Æµ‡Æø‡Æ≤‡Øà‡Æï‡Æ≥‡Øç ‡ÆÆ‡Øã‡Æö‡ÆÆ‡Æü‡Øà‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Æ©.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Tamil:</b> ‡ÆÖ‡ÆÆ‡ØÜ‡Æ∞‡Æø‡Æï‡Øç‡Æï‡Ææ‡Æµ‡Æø‡Æ≤‡Øç ‡Æµ‡Æü‡Øç‡Æü‡Æø ‡Æµ‡Æø‡Æï‡Æø‡Æ§‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡ÆÖ‡Æ§‡Æø‡Æï‡Æ∞‡Æø‡Æ§‡Øç‡Æ§‡Ææ‡Æ≤‡Øç, ‡Æâ‡Æ£‡Æ∞‡Øç‡Æµ‡ØÅ ‡Æö‡Æ®‡Øç‡Æ§‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡Æ§‡Æô‡Øç‡Æï ‡Æµ‡Æø‡Æ≤‡Øà‡Æï‡Æ≥‡Øç ‡ÆÆ‡Øã‡Æö‡ÆÆ‡Æü‡Øà‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Æ©.<br><b>Telugu Translation:</b> ‡∞Ø‡±Å‡∞®‡±à‡∞ü‡±Ü‡∞°‡±ç ‡∞∏‡±ç‡∞ü‡±á‡∞ü‡±ç‡∞∏‡±ç ‡∞≤‡±ã ‡∞µ‡∞°‡±ç‡∞°‡±Ä ‡∞∞‡±á‡∞ü‡±ç‡∞≤‡±Å ‡∞™‡±Ü‡∞∞‡∞ó‡∞°‡∞Ç ‡∞µ‡∞≤‡±ç‡∞≤ ‡∞¨‡∞Ç‡∞ó‡∞æ‡∞∞‡∞Ç ‡∞ß‡∞∞‡∞≤‡±Å ‡∞Æ‡∞∞‡∞ø‡∞Ç‡∞§ ‡∞¶‡∞ø‡∞ó‡∞ú‡∞æ‡∞∞‡∞ø‡∞™‡±ã‡∞§‡∞æ‡∞Ø‡∞ø.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m             display(HTML(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<p style=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor: red;\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>Error during translation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</p>\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Run the translator\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m translate_interactively()\n",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m, in \u001b[0;36mtranslate_interactively\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter Tamil text below to translate to Telugu (type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to stop):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Get input from user\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTamil Input: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Check for exit condition\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nmt/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1286\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1287\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/nmt/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBart50TokenizerFast, AutoModelForSeq2SeqLM\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "MODEL_PATH = \"./mbart_finetuned3\"\n",
    "mbart_saved_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device)\n",
    "mbart_saved_tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_PATH, src_lang=\"ta_IN\", tgt_lang=\"te_IN\")\n",
    "\n",
    "# Translation function\n",
    "def mbart_translate_text(input_text, debug=False):\n",
    "    inputs = mbart_saved_tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).to(device)\n",
    "    if debug:\n",
    "        print(\"Tokenized Input IDs:\", inputs[\"input_ids\"].tolist())\n",
    "    outputs = mbart_saved_model.generate(\n",
    "        **inputs,\n",
    "        max_length=256,\n",
    "        min_length=10,\n",
    "        num_beams=5,\n",
    "        early_stopping=False,\n",
    "        length_penalty=1.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        forced_bos_token_id=mbart_saved_tokenizer.lang_code_to_id[\"te_IN\"]\n",
    "    )\n",
    "    if debug:\n",
    "        print(\"Raw Output IDs:\", outputs[0].tolist())\n",
    "        print(\"Decoded with special tokens:\", mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=False))\n",
    "    decoded_output = mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output.strip()\n",
    "\n",
    "# Interactive translation function for Jupyter\n",
    "def translate_interactively():\n",
    "    display(HTML(\"<h3>Tamil to Telugu Translator</h3>\"))\n",
    "    print(\"Enter Tamil text below to translate to Telugu (type 'exit' to stop):\")\n",
    "    \n",
    "    while True:\n",
    "        # Get input from user\n",
    "        user_input = input(\"Tamil Input: \").strip()\n",
    "        \n",
    "        # Check for exit condition\n",
    "        if user_input.lower() == \"exit\":\n",
    "            display(HTML(\"<p style='color: green;'>Exiting translator...</p>\"))\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            display(HTML(\"<p style='color: red;'>Please enter some text.</p>\"))\n",
    "            continue\n",
    "        \n",
    "        # Translate and display result\n",
    "        try:\n",
    "            translated_text = mbart_translate_text(user_input, debug=False)  # Set debug=True for detailed output\n",
    "            display(HTML(f\"<p><b>Tamil:</b> {user_input}<br><b>Telugu Translation:</b> {translated_text}</p>\"))\n",
    "        except Exception as e:\n",
    "            display(HTML(f\"<p style='color: red;'>Error during translation: {e}</p>\"))\n",
    "\n",
    "# Run the translator\n",
    "translate_interactively()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ba471-7396-48f5-abd2-7299f2efe76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m√ó\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m‚ï∞‚îÄ>\u001b[0m \u001b[31m[33 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 137, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     backend = _build_backend()\n",
      "  \u001b[31m   \u001b[0m               ^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 70, in _build_backend\n",
      "  \u001b[31m   \u001b[0m     obj = import_module(mod_path)\n",
      "  \u001b[31m   \u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/mca/anaconda3/envs/nmt/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "  \u001b[31m   \u001b[0m     return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1381, in _gcd_import\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1354, in _find_and_load\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1304, in _find_and_load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1381, in _gcd_import\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1354, in _find_and_load\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1325, in _find_and_load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 929, in _load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 994, in exec_module\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-i08akry_/overlay/lib/python3.12/site-packages/setuptools/__init__.py\", line 16, in <module>\n",
      "  \u001b[31m   \u001b[0m     import setuptools.version\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-i08akry_/overlay/lib/python3.12/site-packages/setuptools/version.py\", line 1, in <module>\n",
      "  \u001b[31m   \u001b[0m     import pkg_resources\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-i08akry_/overlay/lib/python3.12/site-packages/pkg_resources/__init__.py\", line 2172, in <module>\n",
      "  \u001b[31m   \u001b[0m     register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "  \u001b[31m   \u001b[0m                     ^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m√ó\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model configuration...\n",
      "Loading model and tokenizer...\n",
      "Loading dataset...\n",
      "Generating translations for 2000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['Telugu', 'Tamil']\n",
      "Dataset shape: (2051, 2)\n",
      "First few rows:\n",
      "                                              Telugu  \\\n",
      "0  \"‡∞ö‡∞ø‡∞§‡±ç‡∞∞‡∞Æ‡±Å \"\"{0}\"\"‡∞®‡±Å ‡∞Ö‡∞™‡±ç‚Äå‡∞≤‡±ã‡∞°‡±Å ‡∞ö‡±á‡∞Ø‡±Å‡∞ö‡±Å‡∞®‡±ç‡∞®‡∞¶‡∞ø ({2} ‡∞≤...   \n",
      "1  \"‡∞¨‡±Å‡∞ï‡±ç‚Äå‡∞Æ‡∞æ‡∞∞‡±ç‡∞ï‡±Å ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Æ‡±Å ‡∞∏‡∞∞‡∞ø‡∞ï‡±Ç‡∞∞‡±ç‡∞™‡∞∞‡∞ø ‡∞¶‡∞∞‡±ç‡∞∂‡∞®‡∞Æ‡±Å‡∞®‡∞Ç‡∞¶‡±Å ‡∞ö...   \n",
      "2             %s ‡∞ó‡∞¶‡∞ø‡∞®‡∞ø ‡∞µ‡∞¶‡∞ø‡∞≤‡∞ø‡∞®‡∞¶‡∞øfoo has left the room   \n",
      "3  '‡∞¨‡∞æ‡∞π‡±Å‡∞¨‡∞≤‡∞ø ' ‡∞∏‡∞ø‡∞®‡∞ø‡∞Æ‡∞æ ‡∞§‡∞∞‡±Å‡∞µ‡∞æ‡∞§ ‡∞™‡±ç‡∞∞‡∞≠‡∞æ‡∞∏‡±ç ‡∞®‡∞ü‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞® ‡∞≠‡∞æ...   \n",
      "4                          ( 1 ‡∞Ø‡±ã‡∞π‡∞æ‡∞®‡±Å 3: 17 ‡∞ö‡∞¶‡∞µ‡∞Ç‡∞°‡∞ø.)   \n",
      "\n",
      "                                               Tamil  \n",
      "0      \"\"\"{0}\"\" ‡Æ™‡Æü‡Æ§‡Øç‡Æ§‡Øà ‡Æ™‡Æ§‡Æø‡Æµ‡Øá‡Æ±‡Øç‡Æ±‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ ({2} ‡Æá‡Æ≤‡Øç {1})\"  \n",
      "1  \"‡Æ§‡Æø‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡Æø ‡Æï‡Ææ‡Æü‡Øç‡Æö‡Æø‡ÆØ‡Æø‡Æ≤‡Øç ‡Æï‡Ææ‡Æü‡Øç‡Æü‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡ÆÆ‡Øç ‡Æ™‡ØÅ‡Æ§‡Øç‡Æ§‡Æï‡Æï‡Øç‡Æï‡ØÅ‡Æ±‡Æø...  \n",
      "2  %s ‡ÆÖ‡Æ±‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡Æá‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ ‡Æµ‡ØÜ‡Æ≥‡Æø‡ÆØ‡Øá‡Æ±‡Æø‡Æ©‡Ææ‡Æ∞‡Øçfoo has left th...  \n",
      "3  '‡Æ™‡Ææ‡Æï‡ØÅ‡Æ™‡Æ≤‡Æø' ‡Æé‡Æ©‡Øç‡Æ± ‡Æ™‡Æø‡Æ∞‡ÆÆ‡Øç‡ÆÆ‡Ææ‡Æ£‡Øç‡Æü ‡Æ™‡Æü‡Æ§‡Øç‡Æ§‡Æø‡Æ±‡Øç‡Æï‡ØÅ ‡Æ™‡Æø‡Æ±‡Æï‡ØÅ ‡Æ™‡Æø‡Æ∞...  \n",
      "4                 ( 1 ‡ÆØ‡Øã‡Æµ‡Ææ‡Æ©‡Øç 3: 17 - ‡Æê ‡Æµ‡Ææ‡Æö‡Æø‡ÆØ‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç.)  \n",
      "Using Tamil column: Tamil\n",
      "Using Telugu column: Telugu\n",
      "Test dataset size: 2051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating translations:   0%|                         | 0/2000 [00:00<?, ?it/s]/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:554: UserWarning: `num_beams` is set to None - defaulting to 1.\n",
      "  warnings.warn(\"`num_beams` is set to None - defaulting to 1.\", UserWarning)\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/utils.py:1283: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Generating translations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [15:13<00:00,  2.19it/s]\n",
      "Computing BLEU score...\n",
      "BLEU Score: 72.77\n",
      "BLEU results saved to ./mBART3_RESULTS/bleu_evaluation_results.csv\n",
      "Computing BERTScore...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ed383d1b634b15815621686973842a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b794d707d7e445193409309e2601190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERTScore F1: 0.8438\n",
      "BERTScore results saved to ./mBART3_RESULTS/bertscore_evaluation_results.csv\n",
      "Computing chrF++ score...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.90 seconds, 512.40 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chrF++ Score: 42.60\n",
      "chrF++ results saved to ./mBART3_RESULTS/chrf_evaluation_results.csv\n",
      "Computing TER score...\n",
      "TER Score: 42.59\n",
      "TER results saved to ./mBART3_RESULTS/ter_evaluation_results.csv\n",
      "Computing COMET score...\n",
      "Downloading COMET model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55db9dd030e94983a385197826925f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Encoder model frozen.\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "Running COMET evaluation...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:14<00:00, 17.24it/s]\n",
      "COMET Score: 0.8888\n",
      "COMET results saved to ./mBART3_RESULTS/comet_evaluation_results.csv\n",
      "Tokenized Input IDs: [[250044, 190574, 4, 37305, 29947, 128251, 73952, 32, 2]]\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:554: UserWarning: `num_beams` is set to None - defaulting to 1.\n",
      "  warnings.warn(\"`num_beams` is set to None - defaulting to 1.\", UserWarning)\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Raw Output IDs: [2, 250045, 9327, 3071, 1886, 89838, 4, 22735, 24722, 91064, 32, 2]\n",
      "Decoded with special tokens: </s>te_IN ‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å?</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION SUMMARY\n",
      "==================================================\n",
      "Number of samples: 2000\n",
      "BLEU Score: 72.77\n",
      "chrF++ Score: 42.60\n",
      "TER Score: 42.59 (lower is better)\n",
      "BERTScore F1: 0.8438\n",
      "COMET Score: 0.8888\n",
      "==================================================\n",
      "\n",
      "Test Translation:\n",
      "Source (Tamil): ‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç, ‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡ØÄ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç?\n",
      "Target (Telugu): ‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 72.76817202342089,\n",
       " 'chrf': 42.59973856124301,\n",
       " 'ter': 42.591251756889136,\n",
       " 'bertscore': 0.843758225440979,\n",
       " 'comet': 0.8887818599641323}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing on csv files\n",
    "# Install required dependencies\n",
    "!pip install -q torch==2.3.1 torchvision==0.18.1\n",
    "!pip install -q transformers==4.41.2 datasets==2.20.0\n",
    "!pip install -q sacrebleu==2.3.1 pandas==2.2.2 numpy==1.25.2 tqdm==4.66.4\n",
    "!pip install -q bert-score==0.3.13\n",
    "!pip install -q protobuf==3.20.3  # Compatible protobuf version\n",
    "!pip install -q indic-nlp-library\n",
    "# Uncomment the line below if you want to use COMET metric\n",
    "# !pip install -q unbabel-comet\n",
    "\n",
    "# Tamil-Telugu Translation Model Evaluation\n",
    "# Combines BLEU, BERTScore, COMET, chrF++ and TER evaluation metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from sacrebleu import corpus_bleu, corpus_chrf, corpus_ter\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# Try importing COMET (optional)\n",
    "try:\n",
    "    from comet import download_model, load_from_checkpoint\n",
    "    comet_available = True\n",
    "except ImportError:\n",
    "    comet_available = False\n",
    "    print(\"COMET not available. Will skip COMET evaluation.\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration - Update these paths for your local environment\n",
    "MODEL_PATH = \"./mbart_finetuned3\"  # Path to your fine-tuned model\n",
    "DATASET_PATH = \"./merged_testing.csv\"  # Path to your test dataset\n",
    "NUM_SAMPLES =2000   # Adjust as needed\n",
    "\n",
    "# Output paths\n",
    "BLEU_OUTPUT_PATH = \"./mBART3_RESULTS/bleu_evaluation_results.csv\"\n",
    "BERTSCORE_OUTPUT_PATH = \"./mBART3_RESULTS/bertscore_evaluation_results.csv\" \n",
    "COMET_OUTPUT_PATH = \"./mBART3_RESULTS/comet_evaluation_results.csv\"\n",
    "CHRF_OUTPUT_PATH = \"./mBART3_RESULTS/chrf_evaluation_results.csv\"\n",
    "TER_OUTPUT_PATH = \"./mBART3_RESULTS/ter_evaluation_results.csv\"\n",
    "\n",
    "# Load the model configuration first\n",
    "logger.info(\"Loading model configuration...\")\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "if hasattr(config, 'generation_config'):\n",
    "    if config.generation_config.early_stopping is None:\n",
    "        config.generation_config.early_stopping = True\n",
    "else:\n",
    "    config.early_stopping = True\n",
    "\n",
    "# Load the model with the fixed config\n",
    "logger.info(\"Loading model and tokenizer...\")\n",
    "# Choose the appropriate model class based on your saved model\n",
    "try:\n",
    "    mbart_saved_model = MBartForConditionalGeneration.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        config=config\n",
    "    ).to(device)\n",
    "except:\n",
    "    # Fall back to AutoModelForSeq2SeqLM if MBartForConditionalGeneration fails\n",
    "    mbart_saved_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        config=config\n",
    "    ).to(device)\n",
    "\n",
    "# Fix generation_config to avoid further issues\n",
    "if hasattr(mbart_saved_model, 'generation_config'):\n",
    "    mbart_saved_model.generation_config.early_stopping = True\n",
    "\n",
    "mbart_saved_tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_PATH, src_lang=\"ta_IN\", tgt_lang=\"te_IN\")\n",
    "\n",
    "# Load the test dataset\n",
    "logger.info(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "print(f\"Dataset columns: {df.columns.tolist()}\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"First few rows:\\n{df.head()}\")\n",
    "\n",
    "# Find the actual column names for Tamil and Telugu sentences\n",
    "tamil_col = None\n",
    "telugu_col = None\n",
    "\n",
    "# Common column name patterns to check\n",
    "tamil_patterns = ['tamil_sentence', 'tamil', 'source', 'src', 'Tamil', 'tamil_text']\n",
    "telugu_patterns = ['telugu_sentence', 'telugu', 'target', 'tgt', 'Telugu', 'telugu_text']\n",
    "\n",
    "for col in df.columns:\n",
    "    if any(pattern.lower() in col.lower() for pattern in tamil_patterns):\n",
    "        tamil_col = col\n",
    "    if any(pattern.lower() in col.lower() for pattern in telugu_patterns):\n",
    "        telugu_col = col\n",
    "\n",
    "if tamil_col is None or telugu_col is None:\n",
    "    raise ValueError(f\"Could not identify Tamil and Telugu columns. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "print(f\"Using Tamil column: {tamil_col}\")\n",
    "print(f\"Using Telugu column: {telugu_col}\")\n",
    "\n",
    "# Select the appropriate columns and drop NaN values\n",
    "df = df[[tamil_col, telugu_col]].dropna()\n",
    "# Rename columns for consistency\n",
    "df = df.rename(columns={tamil_col: 'tamil_sentence', telugu_col: 'telugu_sentence'})\n",
    "test_dataset = Dataset.from_pandas(df)\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Function to tokenize Telugu text using IndicNLP\n",
    "def indic_tokenize_text(text):\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\"\n",
    "    return ' '.join(indic_tokenize.trivial_tokenize(text, lang='te'))\n",
    "\n",
    "# Translation function (optimized parameters)\n",
    "def mbart_translate_text(input_text, debug=False):\n",
    "    inputs = mbart_saved_tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).to(device)\n",
    "    if debug:\n",
    "        logger.info(f\"Tokenized Input IDs: {inputs['input_ids'].tolist()}\")\n",
    "    outputs = mbart_saved_model.generate(\n",
    "        **inputs,\n",
    "        max_length=256,\n",
    "        min_length=10,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        length_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        forced_bos_token_id=mbart_saved_tokenizer.lang_code_to_id[\"te_IN\"]\n",
    "    )\n",
    "    if debug:\n",
    "        logger.info(f\"Raw Output IDs: {outputs[0].tolist()}\")\n",
    "        logger.info(f\"Decoded with special tokens: {mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=False)}\")\n",
    "    decoded_output = mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output.strip()\n",
    "\n",
    "# Generate translations for evaluation\n",
    "def generate_translations(dataset, num_samples=NUM_SAMPLES):\n",
    "    sources = []\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    # Limit to num_samples or use full dataset\n",
    "    test_data = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    logger.info(f\"Generating translations for {len(test_data)} samples\")\n",
    "    \n",
    "    for example in tqdm(test_data, desc=\"Generating translations\"):\n",
    "        input_text = example[\"tamil_sentence\"]\n",
    "        reference = example[\"telugu_sentence\"]\n",
    "        \n",
    "        try:\n",
    "            hypothesis = mbart_translate_text(input_text, debug=False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error translating '{input_text}': {e}\")\n",
    "            hypothesis = \"\"\n",
    "            \n",
    "        sources.append(input_text)\n",
    "        references.append(reference)\n",
    "        hypotheses.append(hypothesis)\n",
    "    \n",
    "    return test_data, sources, references, hypotheses\n",
    "\n",
    "# Compute BLEU score\n",
    "def compute_bleu(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing BLEU score...\")\n",
    "    \n",
    "    # Tokenize for BLEU calculation\n",
    "    tokenized_hypotheses = [indic_tokenize_text(hyp) for hyp in hypotheses]\n",
    "    tokenized_references = [[indic_tokenize_text(ref)] for ref in references]\n",
    "    \n",
    "    # Compute SacreBLEU score\n",
    "    bleu = corpus_bleu(tokenized_hypotheses, tokenized_references, tokenize='none')  # Tokenization already done\n",
    "    bleu_score = bleu.score\n",
    "    logger.info(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame({\n",
    "        \"tamil_sentence\": sources,\n",
    "        \"telugu_sentence\": references,\n",
    "        \"telugu_hypothesis\": hypotheses,\n",
    "        \"bleu_score\": [bleu_score] * len(sources)  # Same corpus BLEU for all rows\n",
    "    })\n",
    "    results_df.to_csv(BLEU_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"BLEU results saved to {BLEU_OUTPUT_PATH}\")\n",
    "    \n",
    "    return bleu_score\n",
    "    \n",
    "# Compute chrF++ score\n",
    "def compute_chrf(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing chrF++ score...\")\n",
    "    \n",
    "    # Prepare references format for chrF++ (list of references for each translation)\n",
    "    refs_list = [[ref] for ref in references]\n",
    "    \n",
    "    # Compute chrF++ score (char order=6, word order=2, beta=2 are standard settings)\n",
    "    chrf = corpus_chrf(hypotheses, refs_list, char_order=6, word_order=2, beta=2)\n",
    "    chrf_score = chrf.score\n",
    "    logger.info(f\"chrF++ Score: {chrf_score:.2f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame({\n",
    "        \"tamil_sentence\": sources,\n",
    "        \"telugu_sentence\": references,\n",
    "        \"telugu_hypothesis\": hypotheses,\n",
    "        \"chrf_score\": [chrf_score] * len(sources)  # Same corpus chrF for all rows\n",
    "    })\n",
    "    results_df.to_csv(CHRF_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"chrF++ results saved to {CHRF_OUTPUT_PATH}\")\n",
    "    \n",
    "    return chrf_score\n",
    "    \n",
    "# Compute TER score (Translation Edit Rate)\n",
    "def compute_ter(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing TER score...\")\n",
    "    \n",
    "    # Prepare references format for TER (list of references for each translation)\n",
    "    refs_list = [[ref] for ref in references]\n",
    "    \n",
    "    # Compute TER score\n",
    "    ter = corpus_ter(hypotheses, refs_list)\n",
    "    ter_score = ter.score\n",
    "    logger.info(f\"TER Score: {ter_score:.2f}\")\n",
    "    \n",
    "    # Note: Lower TER is better (it's an error rate)\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame({\n",
    "        \"tamil_sentence\": sources,\n",
    "        \"telugu_sentence\": references,\n",
    "        \"telugu_hypothesis\": hypotheses,\n",
    "        \"ter_score\": [ter_score] * len(sources)  # Same corpus TER for all rows\n",
    "    })\n",
    "    results_df.to_csv(TER_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"TER results saved to {TER_OUTPUT_PATH}\")\n",
    "    \n",
    "    return ter_score\n",
    "\n",
    "# Compute BERTScore\n",
    "def compute_bertscore(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing BERTScore...\")\n",
    "    \n",
    "    # Compute BERTScore\n",
    "    P, R, F1 = bert_score(\n",
    "        hypotheses,\n",
    "        references,\n",
    "        lang=\"te\",  # Telugu language code\n",
    "        model_type=\"bert-base-multilingual-cased\",\n",
    "        device=device,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Average F1 score\n",
    "    avg_f1 = F1.mean().item()\n",
    "    logger.info(f\"BERTScore F1: {avg_f1:.4f}\")\n",
    "    \n",
    "    # Store per-sentence F1 scores\n",
    "    bert_f1_scores = [f1.item() for f1 in F1]\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame({\n",
    "        \"tamil_sentence\": sources,\n",
    "        \"telugu_sentence\": references,\n",
    "        \"telugu_hypothesis\": hypotheses,\n",
    "        \"bertscore_f1\": bert_f1_scores\n",
    "    })\n",
    "    results_df.to_csv(BERTSCORE_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"BERTScore results saved to {BERTSCORE_OUTPUT_PATH}\")\n",
    "    \n",
    "    return avg_f1\n",
    "\n",
    "# Compute COMET score\n",
    "def compute_comet(test_data, sources, references, hypotheses):\n",
    "    if not comet_available:\n",
    "        logger.warning(\"COMET not available. Skipping COMET evaluation.\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"Computing COMET score...\")\n",
    "    \n",
    "    # Load COMET model\n",
    "    logger.info(\"Downloading COMET model...\")\n",
    "    model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "    model = load_from_checkpoint(model_path)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Prepare data for COMET\n",
    "    data = []\n",
    "    for src, hyp, ref in zip(sources, hypotheses, references):\n",
    "        data.append({\n",
    "            \"src\": src,\n",
    "            \"mt\": hyp,\n",
    "            \"ref\": ref\n",
    "        })\n",
    "    \n",
    "    # Compute scores\n",
    "    logger.info(\"Running COMET evaluation...\")\n",
    "    model_output = model.predict(data, batch_size=8, gpus=1 if device == \"cuda\" else 0)\n",
    "    comet_scores = model_output.scores\n",
    "    avg_comet = model_output.system_score\n",
    "    \n",
    "    logger.info(f\"COMET Score: {avg_comet:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame({\n",
    "        \"tamil_sentence\": sources,\n",
    "        \"telugu_sentence\": references,\n",
    "        \"telugu_hypothesis\": hypotheses,\n",
    "        \"comet_score\": comet_scores\n",
    "    })\n",
    "    results_df.to_csv(COMET_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"COMET results saved to {COMET_OUTPUT_PATH}\")\n",
    "    \n",
    "    return avg_comet\n",
    "\n",
    "# Main evaluation function\n",
    "def evaluate_model():\n",
    "    # Generate translations\n",
    "    test_data, sources, references, hypotheses = generate_translations(test_dataset, NUM_SAMPLES)\n",
    "    \n",
    "    # Compute metrics\n",
    "    bleu_score = compute_bleu(test_data, sources, references, hypotheses)\n",
    "    bertscore_f1 = compute_bertscore(test_data, sources, references, hypotheses)\n",
    "    chrf_score = compute_chrf(test_data, sources, references, hypotheses)\n",
    "    ter_score = compute_ter(test_data, sources, references, hypotheses)\n",
    "    \n",
    "    comet_score = None\n",
    "    if comet_available:\n",
    "        comet_score = compute_comet(test_data, sources, references, hypotheses)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Number of samples: {len(sources)}\")\n",
    "    print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "    print(f\"chrF++ Score: {chrf_score:.2f}\")\n",
    "    print(f\"TER Score: {ter_score:.2f} (lower is better)\")\n",
    "    print(f\"BERTScore F1: {bertscore_f1:.4f}\")\n",
    "    if comet_score is not None:\n",
    "        print(f\"COMET Score: {comet_score:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test a single translation\n",
    "    test_input = \"‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç, ‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡ØÄ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç?\"  # \"Hello, how are you?\"\n",
    "    translated_text = mbart_translate_text(test_input, debug=True)\n",
    "    print(f\"\\nTest Translation:\")\n",
    "    print(f\"Source (Tamil): {test_input}\")\n",
    "    print(f\"Target (Telugu): {translated_text}\")\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"chrf\": chrf_score,\n",
    "        \"ter\": ter_score,\n",
    "        \"bertscore\": bertscore_f1,\n",
    "        \"comet\": comet_score\n",
    "    }\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207b0263-d223-48d4-b0ce-1191293c657f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
